{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install --force-reinstall --no-cache-dir tenacity --user\n",
    "!pip install \"ibm-watsonx-ai==1.0.4\" --user\n",
    "!pip install \"ibm-watson-machine-learning==1.0.357\" --user\n",
    "!pip install \"langchain-ibm==0.1.7\" --user\n",
    "!pip install \"langchain-community==0.2.1\" --user\n",
    "!pip install \"langchain-experimental==0.0.59\" --user\n",
    "!pip install \"langchainhub==0.1.17\" --user\n",
    "!pip install \"langchain==0.2.1\" --user\n",
    "!pip install \"pypdf==4.2.0\" --user\n",
    "!pip install \"chromadb == 0.4.24\" --user"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM"
   ],
   "id": "c4dbbc493eb522e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_id = 'mistralai/mixtral-8x7b-instruct-v01'\n",
    "\n",
    "parameters = {\n",
    "    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output\n",
    "    GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses\n",
    "}\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
    "}\n",
    "\n",
    "project_id = \"skills-network\"\n",
    "\n",
    "model = ModelInference(\n",
    "    model_id=model_id,\n",
    "    params=parameters,\n",
    "    credentials=credentials,\n",
    "    project_id=project_id\n",
    ")"
   ],
   "id": "6b24e35f8c329442"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "msg = model.generate(\"In today's sales meeting, we \")\n",
    "print(msg['results'][0]['generated_text'])"
   ],
   "id": "a89c9a95c089e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "mixtral_llm = WatsonxLLM(model = model)",
   "id": "4698f236a30762f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(mixtral_llm.invoke(\"Who is man's best friend?\"))",
   "id": "7dab1cc420d0a4a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage",
   "id": "f7eb09f3dd1c9880"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "msg = mixtral_llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful AI bot that assists a user in choosing the perfect book to read in one short sentence\"),\n",
    "        HumanMessage(content=\"I enjoy mystery novels, what should I read?\")\n",
    "    ]\n",
    ")"
   ],
   "id": "b5111b8efafdb45f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(msg)",
   "id": "85aebf02294f6e35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "msg = mixtral_llm.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a supportive AI bot that suggests fitness activities to a user in one short sentence\"),\n",
    "        HumanMessage(content=\"I like high-intensity workouts, what should I do?\"),\n",
    "        AIMessage(content=\"You should try a CrossFit class\"),\n",
    "        HumanMessage(content=\"How often should I attend?\")\n",
    "    ]\n",
    ")"
   ],
   "id": "951fac76f8428a70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(msg)",
   "id": "a0601fd6e146b04e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "msg = mixtral_llm.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"What month follows June?\")\n",
    "    ]\n",
    ")"
   ],
   "id": "36f97c0de55b146a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain_core.prompts import PromptTemplate",
   "id": "93f5b96281a68ea8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt = PromptTemplate.from_template(\"Tell me one {adjective} joke about {topic}\")\n",
    "input_ = {\"adjective\": \"funny\", \"topic\": \"cats\"}  # create a dictionary to store the corresponding input to placeholders in prompt template"
   ],
   "id": "c7689667e50cddd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "prompt.invoke(input_)",
   "id": "6df18a3d85d15ab9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "input_ = {\"topic\": \"cats\"}\n",
    "\n",
    "prompt.invoke(input_)"
   ],
   "id": "91baf257c4b1eb3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "input_ = {\"msgs\": [HumanMessage(content=\"What is the day after Tuesday?\")]}\n",
    "\n",
    "prompt.invoke(input_)"
   ],
   "id": "247adf8c262333c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chain = prompt | mixtral_llm\n",
    "response = chain.invoke(input = input_)\n",
    "print(response)"
   ],
   "id": "cf7413acb6e2ae3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Examples of a pretend task of creating antonyms.\n",
    "examples = [\n",
    "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
    "    {\"input\": \"tall\", \"output\": \"short\"},\n",
    "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
    "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
    "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\",\n",
    ")\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=25,  # The maximum length that the formatted examples should be.\n",
    ")\n",
    "dynamic_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    suffix=\"Input: {adjective}\\nOutput:\",\n",
    "    input_variables=[\"adjective\"],\n",
    ")"
   ],
   "id": "f918315957cd7297"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(dynamic_prompt.format(adjective=\"big\"))",
   "id": "384e1d8a2cf1df33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "long_string = \"big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else\"\n",
    "print(dynamic_prompt.format(adjective=long_string))"
   ],
   "id": "9e45448777ca3861"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ],
   "id": "63a4f9022e8edfe5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")"
   ],
   "id": "fd138a6fa65caf35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "output_parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt | mixtral_llm | output_parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ],
   "id": "d34808f7aae10c5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query. {format_instructions}\\nList five {subject}.\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt | mixtral_llm | output_parser"
   ],
   "id": "c52b84347a6a05b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "chain.invoke({\"subject\": \"ice cream flavors\"})",
   "id": "8556d885a262887b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain_core.documents import Document",
   "id": "cf71f586667f82b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Document(page_content=\"\"\"Python is an interpreted high-level general-purpose programming language.\n",
    "                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\"\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"About Python\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })"
   ],
   "id": "cd7a5dc9067b08a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Document(page_content=\"\"\"Python is an interpreted high-level general-purpose programming language.\n",
    "                        Python's design philosophy emphasizes code readability with its notable use of significant indentation.\"\"\")"
   ],
   "id": "c31fd6c7eb1af6e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain_community.document_loaders import PyPDFLoader",
   "id": "ec19ee18565cca4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "loader = PyPDFLoader(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf\")",
   "id": "e7aac94f25f5de2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "document = loader.load()",
   "id": "369a7048c19f6659"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "document[2]",
   "id": "ba56b6741f9c2d29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(document[1].page_content[:1000])  # print the page 1's first 1000 tokens",
   "id": "6052b97bcf06f608"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain_community.document_loaders import WebBaseLoader",
   "id": "6f6c347e310dec09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "loader = WebBaseLoader(\"https://python.langchain.com/v0.2/docs/introduction/\")",
   "id": "d8a5e93dd41b086e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "web_data = loader.load()",
   "id": "b12ba746d00e7426"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(web_data[0].page_content[:1000])",
   "id": "963d6f5d2c3aecb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain.text_splitter import CharacterTextSplitter",
   "id": "813cfa0a2d63b8ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator=\"\\n\")  # define chunk_size which is length of characters, and also separator.\n",
    "chunks = text_splitter.split_documents(document)\n",
    "print(len(chunks))"
   ],
   "id": "5a402b63fde69bbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "chunks[5].page_content   # take a look at any chunk's page content",
   "id": "9783bce1579180cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames\n",
    "\n",
    "embed_params = {\n",
    "    EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,\n",
    "    EmbedTextParamsMetaNames.RETURN_OPTIONS: {\"input_text\": True},\n",
    "}"
   ],
   "id": "539571bcac21d3f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_ibm import WatsonxEmbeddings\n",
    "\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/slate-125m-english-rtrvr\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=\"skills-network\",\n",
    "    params=embed_params,\n",
    ")"
   ],
   "id": "ff769a4ec119860d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "texts = [text.page_content for text in chunks]\n",
    "\n",
    "embedding_result = watsonx_embedding.embed_documents(texts)\n",
    "embedding_result[0][:5]"
   ],
   "id": "18f974ff226bf39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain.vectorstores import Chroma",
   "id": "633c5992aaf529f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "docsearch = Chroma.from_documents(chunks, watsonx_embedding)",
   "id": "bd931a7a1e5fe71f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "query = \"Langchain\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ],
   "id": "24a112159def85b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "retriever = docsearch.as_retriever()",
   "id": "11fdb474e5fbc185"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "docs = retriever.invoke(\"Langchain\")",
   "id": "daa51b81db4ade61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "docs[0]",
   "id": "ffbe5cb7020d159b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore"
   ],
   "id": "ff8fdb8080dd7de0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set two splitters. One is with big chunk size (parent) and one is with small chunk size (child)\n",
    "parent_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=20, separator='\\n')\n",
    "child_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=20, separator='\\n')\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"split_parents\", embedding_function=watsonx_embedding\n",
    ")\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ],
   "id": "c498df9ea2c90025"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ],
   "id": "bb01a9c92dad9df0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "retriever.add_documents(document)",
   "id": "fec3bbf6566fdb5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(list(store.yield_keys()))",
   "id": "73139400b8680d39"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sub_docs = vectorstore.similarity_search(\"Langchain\")",
   "id": "e02de8a21ee36ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(sub_docs[0].page_content)",
   "id": "bb31ce1d0ae55b59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "retrieved_docs = retriever.invoke(\"Langchain\")",
   "id": "d5eb84790c58dc94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(retrieved_docs[0].page_content)",
   "id": "ae747661c0ff2b07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain.chains import RetrievalQA",
   "id": "8be4d69498dd8138"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=mixtral_llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=docsearch.as_retriever(),\n",
    "                                 return_source_documents=False)\n",
    "query = \"what is this paper discussing?\"\n",
    "qa.invoke(query)"
   ],
   "id": "249b9dc58b4d84d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain.memory import ChatMessageHistory",
   "id": "16c1bbcee3899a66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chat = mixtral_llm\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_ai_message(\"hi!\")\n",
    "\n",
    "history.add_user_message(\"what is the capital of France?\")"
   ],
   "id": "8b382f48d7909878"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "history.messages",
   "id": "adae3b15deb7b211"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ai_response = chat.invoke(history.messages)\n",
    "ai_response"
   ],
   "id": "57c062d17b4f9aaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "history.add_ai_message(ai_response)\n",
    "history.messages"
   ],
   "id": "88cc6cd76bee2c0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ],
   "id": "9492588c41f19a0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=mixtral_llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ],
   "id": "4dacaa419046e3a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "conversation.invoke(input=\"Hello, I am a little cat. Who are you?\")",
   "id": "f8b479ff157b5e09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "conversation.invoke(input=\"What can you do?\")",
   "id": "af72eb01d616be8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "conversation.invoke(input=\"Who am I?.\")",
   "id": "f9085a1aa4642eaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain.chains import LLMChain",
   "id": "6008c18ecf8fdaae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "                {location}\n",
    "\n",
    "                YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['location'])\n",
    "\n",
    "# chain 1\n",
    "location_chain = LLMChain(llm=mixtral_llm, prompt=prompt_template, output_key='meal')"
   ],
   "id": "a368bb0089dfc37c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "location_chain.invoke(input={'location':'China'})",
   "id": "6828caef0b4c4d2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain.chains import SequentialChain",
   "id": "8e6fee7205c5a041"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "template = \"\"\"Given a meal {meal}, give a short and simple recipe on how to make that dish at home.\n",
    "\n",
    "                YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['meal'])\n",
    "\n",
    "# chain 2\n",
    "dish_chain = LLMChain(llm=mixtral_llm, prompt=prompt_template, output_key='recipe')"
   ],
   "id": "7a0ab2b9bfd1ac77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "template = \"\"\"Given the recipe {recipe}, estimate how much time I need to cook it.\n",
    "\n",
    "                YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=template, input_variables=['recipe'])\n",
    "\n",
    "# chain 3\n",
    "recipe_chain = LLMChain(llm=mixtral_llm, prompt=prompt_template, output_key='time')"
   ],
   "id": "33c1e57354ef8c98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# overall chain\n",
    "overall_chain = SequentialChain(chains=[location_chain, dish_chain, recipe_chain],\n",
    "                                      input_variables=['location'],\n",
    "                                      output_variables=['meal', 'recipe', 'time'],\n",
    "                                      verbose= True)"
   ],
   "id": "bf307164bd7c5f51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from pprint import pprint",
   "id": "8c7d11eb61eb3e58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pprint(overall_chain.invoke(input={'location':'China'}))",
   "id": "258e7919f884e9eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain.chains.summarize import load_summarize_chain",
   "id": "b0fbefd2717d2a17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chain = load_summarize_chain(llm=mixtral_llm, chain_type=\"stuff\", verbose=False)\n",
    "response = chain.invoke(web_data)"
   ],
   "id": "1e8ceae437218f96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(response['output_text'])",
   "id": "82fa383ebfdea021"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T18:33:26.548344Z",
     "start_time": "2025-03-05T18:33:24.895700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain_experimental.utilities import PythonREPL"
   ],
   "id": "8d6688e1ae9db6c3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python\\projects\\CtxEnginPromptTemplLangChain\\.venv\\Lib\\site-packages\\langchain\\chains\\api\\base.py:56: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_community.utilities.requests import TextRequestsWrapper\n",
      "D:\\python\\projects\\CtxEnginPromptTemplLangChain\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_generate_schema.py:775: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `TextRequestsWrapper` to V2.\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_experimental'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01magents\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Tool\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_experimental\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutilities\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m PythonREPL\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'langchain_experimental'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "python_repl = PythonREPL()",
   "id": "2c1d0550807873c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "python_repl.run(\"a = 3; b = 1; print(a+b)\")",
   "id": "e9bf6448029a859c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain_experimental.tools import PythonREPLTool",
   "id": "3ab61a219322abb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tools = [PythonREPLTool()]",
   "id": "cba295cbf38fb355"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.agents import create_react_agent\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor"
   ],
   "id": "ec57a23291dbac9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "instructions = \"\"\"You are an agent designed to write and execute python code to answer questions.\n",
    "You have access to a python REPL, which you can use to execute python code.\n",
    "If you get an error, debug your code and try again.\n",
    "Only use the output of your code to answer the question.\n",
    "You might know the answer without running any code, but you should still run the code to get the answer.\n",
    "If it does not seem like you can write code to answer the question, just return \"I don't know\" as the answer.\n",
    "\"\"\"\n",
    "\n",
    "# here you will use the prompt directly from the langchain hub\n",
    "base_prompt = hub.pull(\"langchain-ai/react-agent-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)"
   ],
   "id": "3a7bbe11d93860c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "agent = create_react_agent(mixtral_llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)  # tools were defined in the toolkit part above"
   ],
   "id": "b548c5ff66113a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "agent_executor.invoke(input = {\"input\": \"What is the 3rd fibonacci number?\"})",
   "id": "f88c8ed22b01013c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model_id = 'meta-llama/llama-3-3-70b-instruct'",
   "id": "db525ec1d644c90a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "parameters = {\n",
    "    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output\n",
    "    GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses\n",
    "}\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
    "}\n",
    "\n",
    "project_id = \"skills-network\"\n",
    "\n",
    "model = ModelInference(\n",
    "    model_id=model_id,\n",
    "    params=parameters,\n",
    "    credentials=credentials,\n",
    "    project_id=project_id\n",
    ")"
   ],
   "id": "418296945bfe9a1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_id = 'meta-llama/llama-3-3-70b-instruct'\n",
    "\n",
    "parameters = {\n",
    "    GenParams.MAX_NEW_TOKENS: 256,  # this controls the maximum number of tokens in the generated output\n",
    "    GenParams.TEMPERATURE: 0.5, # this randomness or creativity of the model's responses\n",
    "}\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\"\n",
    "}\n",
    "\n",
    "project_id = \"skills-network\"\n",
    "\n",
    "model = ModelInference(\n",
    "    model_id=model_id,\n",
    "    params=parameters,\n",
    "    credentials=credentials,\n",
    "    project_id=project_id\n",
    ")"
   ],
   "id": "b31b4f2a51024735"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator=\".\")  # define chunk_size which is length of characteres, and also separator.\n",
    "chunks = text_splitter.split_documents(document)\n",
    "print(len(chunks))\n",
    "\n",
    "print(chunks[5].page_content)"
   ],
   "id": "2696bd797eb87d68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Your code here\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ZNoKMJ9rssJn-QbJ49kOzA/student-mat.csv\"\n",
    ")\n",
    "\n",
    "agent = create_pandas_dataframe_agent(\n",
    "    mixtral_llm,\n",
    "    df,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True\n",
    ")\n",
    "\n",
    "response = agent.invoke(\"How many rows in the dataframe?\",handle_parsing_errors=True)\n",
    "\n",
    "print(response['output'])"
   ],
   "id": "c6b261aa9b0097f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
